{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn                     # neural networks\n",
    "import math\n",
    "\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transport equation in segregation direction, assuming imcompressible flow\n",
    "$$ \\frac{\\partial c_i}{\\partial t} +\\frac{\\partial ({w_{seg,i} c_i)}}{\\partial z} = \\frac{\\partial }{\\partial z} ( D \\frac{\\partial c_i }{\\partial z} ) $$\n",
    "\n",
    "Segregation velocity model:\n",
    "$$ w_{seg,i} = 0.26 \\ln R_d \\dot \\gamma (1-c_i)$$\n",
    "\n",
    "\n",
    "Diffusion coefficient:\n",
    "$$ D = 0.042 \\dot\\gamma (c_ld_l + c_s d_s)^2$$\n",
    "\n",
    "<!-- $$ D = 0.0084 (1+c_l)$$ -->\n",
    "\n",
    "<!-- So final pde becomes:\n",
    "$$ \\frac{\\partial c_i}{\\partial t} +\\frac{\\partial ({18.2 (1-c_i) c_i)}}{\\partial z} = \\frac{\\partial }{\\partial z} ( 0.0084 (1+c_l) \\frac{\\partial c_i }{\\partial z} ) $$\n",
    "\n",
    "$$ \\frac{\\partial c_i}{\\partial t} +  (18.2 -36.4 c_i)\\frac{\\partial c_i}{\\partial z} =  0.0084(1+c_i) \\frac{\\partial ^2 c_i}{\\partial z^2} + 0.0084 (\\frac{\\partial  c_i}{\\partial z})^2$$ -->\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        # 6 layer neural network\n",
    "        super(Net, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(100)\n",
    "        self.ln2 = nn.LayerNorm(100)\n",
    "        self.ln3 = nn.LayerNorm(100)\n",
    "        self.ln4 = nn.LayerNorm(100)\n",
    "\n",
    "        self.hidden_layer1 = nn.Linear(1,100)\n",
    "        self.hidden_layer2 = nn.Linear(100,100)\n",
    "        self.hidden_layer3 = nn.Linear(100,100)\n",
    "        self.hidden_layer4 = nn.Linear(100,100)\n",
    "        self.hidden_layer5 = nn.Linear(100,100)\n",
    "        self.output_layer = nn.Linear(100,1)\n",
    "        self.mse=torch.nn.MSELoss()\n",
    "        \n",
    "        # particle properties in S.I unit\n",
    "        self.rd=2\n",
    "        self.dl=0.004\n",
    "        self.rho=1000\n",
    "        self.c_diffusion=0.042\n",
    "        self.ds=self.dl/self.rd\n",
    "        self.rds=1/self.rd\n",
    "        self.ml=4/3*math.pi*0.002**3*self.rho\n",
    "        self.ms=4/3*math.pi*0.001**3*self.rho\n",
    "        \n",
    "        \n",
    "        # flow configuration (uniform shear)\n",
    "        self.gamma=100\n",
    "        self.phi=0.55\n",
    "        self.g=9.81\n",
    "        self.h0=0.01\n",
    "        self.p0=self.h0*self.rho*self.g*self.phi\n",
    "\n",
    "        # segregation force calculation\n",
    "        self.theta=torch.tensor(np.cos(0), dtype=torch.float32, requires_grad=True).to(device)  \n",
    "        #  Duan et al. 2024\n",
    "        _intruder_l=(1-1.43*np.exp(-self.rd/0.92))*(1+3.55*np.exp(-self.rd/2.94))*self.phi\n",
    "        _intruder_s=(1-1.43*np.exp(-self.rds/0.92))*(1+3.55*np.exp(-self.rds/2.94))*self.phi\n",
    "\n",
    "        self.intruder_l=torch.tensor(_intruder_l, dtype=torch.float32, requires_grad=True).to(device) \n",
    "        self.intruder_s=torch.tensor(_intruder_s, dtype=torch.float32, requires_grad=True).to(device) \n",
    "\n",
    "    def forward(self, z):\n",
    "        # if time dimensino is considered, concatenated first, i.e. torch.cat([z,t],axis=1) \n",
    "        layer1_out = self.ln1(torch.sigmoid(self.hidden_layer1(z)))\n",
    "        layer2_out = self.ln2(torch.sigmoid(self.hidden_layer2(layer1_out)))\n",
    "        layer3_out = self.ln3(torch.sigmoid(self.hidden_layer3(layer2_out)))\n",
    "        layer4_out = self.ln4(torch.sigmoid(self.hidden_layer4(layer3_out)))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "    \n",
    "    def loss(self):\n",
    "\n",
    "\n",
    "        # PDE loss    \n",
    "        z_collocation = np.random.uniform(low=0.0, high=0.1, size=(10001,1))\n",
    "\n",
    "        z = torch.tensor(z_collocation, dtype=torch.float32, requires_grad=True).to(device) \n",
    "\n",
    "        c = self(z) \n",
    "        \n",
    "        p=self.rho*self.phi*self.g*z+self.p0\n",
    "        inert=self.gamma*(c*0.004+(1-c)*(0.004/self.rd))/torch.sqrt(p/self.rho);\n",
    "        mu_eff=0.364+(0.772-0.364)/(0.434/inert+1)\n",
    "        eta=mu_eff*p/self.gamma\n",
    "\n",
    "        mixture_l=(self.intruder_l-self.theta)*torch.tanh((self.theta-self.intruder_s)/(self.intruder_l-self.theta)*self.ml/self.ms*(1-c)/c)\n",
    "        mixture_s=-(self.intruder_l-self.theta)*c/(1-c)*self.ms/self.ml*torch.tanh((self.theta-self.intruder_s)/(self.intruder_l-self.theta)*self.ml/self.ms*(1-c)/c)\n",
    "        \n",
    "\n",
    "        cd=(2-7*math.exp(-2.6*self.rd))+0.57*inert*self.rd\n",
    "\n",
    "        wseg=mixture_l*self.ml*self.g / (cd*math.pi*eta*0.004)\n",
    "\n",
    "        c_z = torch.autograd.grad(c.sum(), z, create_graph=True)[0]\n",
    "        \n",
    "        # Duan et al. 2024  \n",
    "        pde=(wseg*c-0.042*self.gamma*torch.square((1-c)*self.ds+c*self.dl)*c_z)*100\n",
    "\n",
    "        # Schlick et al. 2015\n",
    "        # simplified with constant diffusion coefficient\n",
    "        # pde = (1/0.1 *(1-c)*c - c_z )*10\n",
    "\n",
    "        target = torch.zeros_like(pde,requires_grad=False)\n",
    "        pde_loss=self.mse(pde,target)\n",
    "\n",
    "\n",
    "        # Mass conservation loss\n",
    "        x_bc = torch.linspace(0, 0.1, 10001,requires_grad=True).to(device)\n",
    "        x_bc = x_bc.unsqueeze(-1)\n",
    "        u_bc = self(x_bc)\n",
    "        u_bc=torch.mean(u_bc)\n",
    "        \n",
    "        target=torch.zeros_like(u_bc,requires_grad=False).to(device)+0.5\n",
    "        mass_loss=self.mse(u_bc,target)\n",
    "   \n",
    "        return mass_loss  + pde_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (3) Training / Fitting\n",
    "iterations = 5000\n",
    "previous_validation_loss = 99999999.0\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "    loss=net.loss()\n",
    "    loss.backward() # This is for computing gradients using backward propagation\n",
    "    optimizer.step() # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n",
    "    if loss.data<1e-6:\n",
    "        break\n",
    "    with torch.autograd.no_grad():\n",
    "    \tprint(epoch,\"Traning Loss:\",loss.data)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d') \n",
    "\n",
    "x=np.arange(0,0.1,0.001)[:,np.newaxis]\n",
    "pt_x = torch.tensor(x, dtype=torch.float32, requires_grad=False).to(device)\n",
    "pt_u = net(pt_x)\n",
    "u=pt_u.data.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(u,x)\n",
    "plt.xlim(0, 1) \n",
    "plt.ylim(0, 0.1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load arrays from the .npz file\n",
    "loaded_data = np.load('solver_lam.npz')\n",
    "lam01 = loaded_data['lam01']\n",
    "lam10 = loaded_data['lam10']\n",
    "lam10 = loaded_data['lam003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('solver_lam.npz', lam01=lam01,lam10=lam10,lam003=u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in loaded_data.values():\n",
    "    \n",
    "plt.plot(loaded_data['lam01'],x,linestyle='--',label=\"$\\lambda=0.1$\")\n",
    "plt.plot(loaded_data['lam003'],x,linestyle='--',label=\"$\\lambda=0.3$\")\n",
    "plt.plot(loaded_data['lam10'],x,linestyle='--',label=\"$\\lambda=10$\")\n",
    "plt.plot(u,x,label=\"full model\")\n",
    "plt.xlim(0, 1) \n",
    "plt.ylim(0, 0.1) \n",
    "plt.xlabel('${c_l}$', fontsize=12)\n",
    "plt.ylabel('$z/h$', fontsize=12)\n",
    "plt.legend()\n",
    "plt.savefig('comparison.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
